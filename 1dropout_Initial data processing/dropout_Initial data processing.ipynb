{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import pickleshare\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unittest.mock import inplace\n",
    "from unittest.mock import patch\n",
    "class PreProcess(object):\n",
    "    def gen_courseid_dict(self,source_path):\n",
    "        # source_path = 'original_data/train/date.csv'\n",
    "\n",
    "        df = pd.read_csv(source_path,usecols=[0])\n",
    "        course_map = pd.factorize(df.course_id)[1]\n",
    "        course_dict = dict(zip(course_map,range(len(course_map))))\n",
    "        print (\"course_dict done...\")\n",
    "        return course_dict\n",
    "\n",
    "    def gen_username_dict(self,source_path_train,source_path_test):\n",
    "        # source_path_train = 'original_data/train/enrollment_train.csv'\n",
    "        # source_path_test = 'original_data/test/enrollment_test.csv'\n",
    "\n",
    "        df = pd.read_csv(source_path_train,usecols=[1])\n",
    "        username_map = pd.factorize(df.username)[1]\n",
    "        username_dict = dict(zip(username_map,range(len(username_map))))\n",
    "\n",
    "        df2 = pd.read_csv(source_path_test,usecols=[1])\n",
    "        username_map2 = pd.factorize(df2.username)[1]\n",
    "        diff = [w for w in username_map2 if w not in username_map]\n",
    "        username_dict2 =dict(zip(diff,np.arange(len(username_map),len(username_map)+len(diff))))\n",
    "\n",
    "        username_dict.update(username_dict2)\n",
    "        print (\"username_dict done...\")\n",
    "        return username_dict\n",
    "\n",
    "    def course_map(self,x):\n",
    "        return self.course_dict[x]\n",
    "\n",
    "    def username_map(self,x):\n",
    "        return self.username_dict[x]\n",
    "\n",
    "    def time_split(self,x):\n",
    "        x = x[:10]\n",
    "        return x\n",
    "\n",
    "    def enrollment_map(self,source_path_train,source_path_test,target_path_train,target_path_test):\n",
    "        print (\"read enrollment_train.csv\")\n",
    "#从两个CSV文件中读取特定的列，对这两列数据进行转换，然后将转换后的数据保存到另外两个CSV文件中\n",
    "        # source_path_train = 'original_data/train/enrollment_train.csv'\n",
    "        # source_path_test = 'original_data/test/enrollment_test.csv'\n",
    "        #\n",
    "        # target_path_train = \"preprocess_data/enrollment_train#.csv\"\n",
    "        # target_path_test = \"preprocess_data/enrollment_test#.csv\"\n",
    "\n",
    "        df1 = pd.read_csv(source_path_train,usecols=[0,1,2],converters={1:self.username_map,2:self.course_map})\n",
    "        df1.to_csv(target_path_train,index=False)\n",
    "        df2 = pd.read_csv(source_path_test,usecols=[0,1,2],converters={1:self.username_map,2:self.course_map})\n",
    "        df2.to_csv(target_path_test,index=False)\n",
    "\n",
    "    def date_map(self,source_path,target_path):\n",
    "        # 从一个CSV文件中读取日期信息，计算两个日期之间的天数差，并将结果保存到另一个CSV文件中\n",
    "        # source_path = 'original_data/train/date.csv'\n",
    "        # target_path = \"preprocess_data/date#.csv\"\n",
    "\n",
    "        print (\"read date.csv\")\n",
    "        df1 = pd.read_csv(source_path,converters={0:self.course_map})\n",
    "        df1[\"day_nums\"]= (pd.to_datetime(df1[\"to\"]) - pd.to_datetime(df1[\"from\"]))\n",
    "        df1[\"day_nums\"] = df1[\"day_nums\"].map(lambda x: x.days)#计算这两个日期之间的差值，并将结果存储在\"day_nums\"这一新列中\n",
    "        df1.to_csv(target_path,index=False)\n",
    "\n",
    "    def log_clean(self,source_path,target_path):\n",
    "        #从CSV文件中读取特定的列，对日期和时间字段进行分割处理，然后将处理后的数据保存到另一个CSV文件中\n",
    "        # source_path = 'original_data/train/log_train.csv'\n",
    "        # target_path = \"preprocess_data/log_train#.csv\"\n",
    "\n",
    "        print (\"read log_train.csv \")\n",
    "        df1 = pd.read_csv(source_path,usecols=[0,1,3]) #change#使用usecols参数指定只读取第0、1、3列。这意味着我们只对CSV文件中的这三列数据感兴趣，其他的列将被忽略。\n",
    "        #source、object列将被忽略\n",
    "        df1[\"date\"] = df1[\"time\"].map(lambda x: x[:10])#对time列时间进行处理\n",
    "        df1[\"time\"] = df1[\"time\"].map(lambda x: x[11:])\n",
    "        df1.to_csv(target_path,index=False)\n",
    "\n",
    "\n",
    "\n",
    "    def course_enrollment(self,source_path_train,source_path_test,source_path_date,target_path_train,target_path_test):\n",
    "        #将两个包含课程注册信息的CSV文件（一个训练集和一个测试集）与一个包含日期信息的CSV文件合并，并将合并后的数据保存到另外两个CSV文件中。\n",
    "        # source_path_train = 'preprocess_data/enrollment_train#.csv'\n",
    "        # source_path_test = 'preprocess_data/enrollment_test#.csv'\n",
    "        # source_path_date = 'preprocess_data/date#.csv'\n",
    "        # target_path_train = \"preprocess_data/course_enrollment_train#.csv\"\n",
    "        # target_path_test = \"preprocess_data/course_enrollment_test#.csv\"\n",
    "\n",
    "        print(\"course_enrollment....\")\n",
    "        df1 = pd.read_csv(source_path_train) #如果不设置index，read_csv读取是默认index(序号)，不是第一列\n",
    "        df2 = pd.read_csv(source_path_test)\n",
    "        df3 = pd.read_csv(source_path_date)\n",
    "        df4 = pd.merge(df1,df3,how=\"left\",left_on=\"course_id\",right_on=\"course_id\")\n",
    "\n",
    "        df5 = pd.merge(df2,df3,how=\"left\",on=\"course_id\")\n",
    "        df4.to_csv(target_path_train,index=False)\n",
    "        df5.to_csv(target_path_test,index=False)\n",
    "\n",
    "    def log_interval(self,source_path_log_train,source_path_enrol_train,target_path):\n",
    "        #计算两个不同CSV文件之间的时间间隔，并将结果保存到另一个CSV文件中\n",
    "        # source_path_log_train = 'preprocess_data/log_train#.csv'\n",
    "        # source_path_enrol_train = 'preprocess_data/course_enrollment_train#.csv'\n",
    "        # target_path = \"preprocess_data/log_train_final#.csv\"\n",
    "        print(\"log_interval....\")\n",
    "        df1 = pd.read_csv(source_path_log_train)\n",
    "        df2 = pd.read_csv(source_path_enrol_train,usecols=[0,3])\n",
    "        df3 = pd.merge(df1,df2,how=\"left\",on=\"enrollment_id\")\n",
    "        df3[\"interval\"]= (pd.to_datetime(df3[\"date\"]) - pd.to_datetime(df3[\"from\"]))\n",
    "        df3[\"interval\"] = df3[\"interval\"].map(lambda x: x.days+1)#interval列现在就包含了两个日期之间的天数差（加1）\n",
    "        df3.drop([\"from\"],axis=1,inplace=True)\n",
    "        df3.to_csv(target_path,index=False)\n",
    "\n",
    "    def enrollment_dropout(self,source_path_enrol_train,source_path_truth,target_path):\n",
    "        #用于处理学生注册和辍学数据\n",
    "        # source_path_enrol_train = 'preprocess_data/enrollment_train#.csv'\n",
    "        # source_path_truth = 'original_data/train/truth_train.csv'\n",
    "        # target_path = \"preprocess_data/enrollment_dropout#.csv\"\n",
    "\n",
    "        print(\"merge_enrollment\")\n",
    "        df1 = pd.read_csv(source_path_enrol_train)\n",
    "        df2 = pd.read_csv(source_path_truth,names=['enrollment_id','dropout'])\n",
    "        df3 = pd.merge(df1,df2,how=\"left\",on=\"enrollment_id\")\n",
    "        gpby_user = df3.groupby(\"username\");\n",
    "        df4 = gpby_user.course_id.count().to_frame()\n",
    "        df4.rename(columns={'course_id':'course_num'}, inplace = True)\n",
    "        gpby_user_dropout = df3.groupby([\"username\",\"dropout\"]);\n",
    "        df5 = gpby_user_dropout.course_id.count().unstack().fillna(0)\n",
    "        df5.rename(columns={0:'nondropout_num', 1:'dropout_num'}, inplace = True)\n",
    "        df5.drop([\"dropout_num\"],axis=1,inplace=True)\n",
    "        df6 = pd.merge(df1,df4,how=\"left\",left_on=\"username\",right_index=True)\n",
    "        df6 = pd.merge(df6,df5,how=\"left\",left_on=\"username\",right_index=True)\n",
    "        df7 = pd.merge(df6,df2,how=\"left\",on=\"enrollment_id\")\n",
    "        df7.to_csv(target_path,index=False)\n",
    "\n",
    "    def data_trans(self):\n",
    "\n",
    "        source_path = 'original_data/train/date.csv'\n",
    "        self.course_dict = self.gen_courseid_dict(source_path)\n",
    "\n",
    "        source_path_train = 'original_data/train/enrollment_train.csv'\n",
    "        source_path_test = 'original_data/test/enrollment_test.csv'\n",
    "        self.username_dict = self.gen_username_dict(source_path_train,source_path_test)\n",
    "\n",
    "        source_path_train='original_data/train/enrollment_train.csv'\n",
    "        source_path_test='original_data/test/enrollment_test.csv'\n",
    "\n",
    "        target_path_train=\"preprocess_data/enrollment_train#.csv\"\n",
    "        target_path_test=\"preprocess_data/enrollment_test#.csv\"\n",
    "        self.enrollment_map(source_path_train,source_path_test,target_path_train,target_path_test)\n",
    "\n",
    "        source_path='original_data/train/date.csv'\n",
    "        target_path=\"preprocess_data/date#.csv\"\n",
    "        self.date_map(source_path,target_path)\n",
    "\n",
    "        source_path='original_data/train/log_train.csv'\n",
    "        target_path=\"preprocess_data/log_train#.csv\"\n",
    "        self.log_clean(source_path,target_path)\n",
    "\n",
    "        source_path_train='preprocess_data/enrollment_train#.csv'\n",
    "        source_path_test='preprocess_data/enrollment_test#.csv'\n",
    "        source_path_date='preprocess_data/date#.csv'\n",
    "        target_path_train=\"preprocess_data/course_enrollment_train#.csv\"\n",
    "        target_path_test=\"preprocess_data/course_enrollment_test#.csv\"\n",
    "        self.course_enrollment(source_path_train,source_path_test,source_path_date,target_path_train,target_path_test)\n",
    "\n",
    "        source_path_log_train='preprocess_data/log_train#.csv'\n",
    "        source_path_enrol_train='preprocess_data/course_enrollment_train#.csv'\n",
    "        target_path=\"preprocess_data/log_train_final#.csv\"\n",
    "        self.log_interval(source_path_log_train,source_path_enrol_train,target_path)\n",
    "\n",
    "        source_path_enrol_train='preprocess_data/enrollment_train#.csv'\n",
    "        source_path_truth='original_data/train/truth_train.csv'\n",
    "        target_path=\"preprocess_data/enrollment_dropout#.csv\"\n",
    "        self.enrollment_dropout(source_path_enrol_train,source_path_truth,target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552079b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import codecs\n",
    "import csv\n",
    "from decimal import *\n",
    "import numpy as np\n",
    "\n",
    "class FeatureEngineering(object):\n",
    "\n",
    "    ##辍学概率=没有辍学的课程数/课程总数\n",
    "    def nondrop_precent(self,source_path,target_path):\n",
    "        #计算非辍学学生的比例，并将结果保存到enrollment_nondrop_precent#.csv文件中\n",
    "        # source_path = 'preprocess_data/enrollment_dropout#.csv'\n",
    "        # target_path = \"feature/enrollment_nondrop_precent#.csv\"\n",
    "\n",
    "        print(\"nondrop_precent...\")\n",
    "        df1 = pd.read_csv(source_path)\n",
    "        df1[\"nondrop_precent\"]=df1[\"nondropout_num\"]/df1[\"course_num\"]\n",
    "        df1.to_csv(target_path,index=False)\n",
    "        \n",
    "    def add(self,x,y):\n",
    "        #接收两个参数x和y，并返回它们的和\n",
    "        return x+y\n",
    "\n",
    "\n",
    "        ##op_character 操作字符\n",
    "    def op_character(self,source_path,target_path):\n",
    "        #分析和提取有关用户操作的特征。具体来说，它处理一个包含用户操作日志的CSV文件，并生成一个新的CSV文件，\n",
    "        # 其中包含每个用户（通过enrollment_id标识）在每个时间间隔（interval）内的操作特征。\n",
    "        # source_path = 'preprocess_data/log_train_final#.csv'\n",
    "        # target_path = \"feature/log_feature#.csv\"\n",
    "\n",
    "        print(\"op_character...\")\n",
    "        df1 = pd.read_csv(source_path)\n",
    "        gpby_enrol = df1.groupby(\"enrollment_id\")#使用groupby方法按enrollment_id对df1进行分组，得到每个用户的操作数据\n",
    "        \n",
    "        enrol_list = list()\n",
    "        interval_list = list()\n",
    "        last_minutes = list()\n",
    "        valid_opnum = list()\n",
    "        all_opnum = list()\n",
    "        \n",
    "        for enrollment_id,group in gpby_enrol:\n",
    "            group.groupby(\"interval\")#对于每个enrollment_id及其对应的操作数据group，\n",
    "                # 进一步按interval进行分组，得到每个用户在每个时间间隔内的操作数据group2\n",
    "\n",
    "            # 提取和计算特征\n",
    "            # 对于每个时间间隔内的操作数据group2，提取以下特征：\n",
    "            # enrollment_id：用户ID。\n",
    "            # interval：时间间隔。\n",
    "            # last_minutes：该时间间隔内第一个和最后一个操作之间的时间差（以分钟为单位）。\n",
    "            # valid_opnum：该时间间隔内有效操作的总数（这里的有效操作包括problem、video、wiki和discussion四种类型）。\n",
    "            # all_opnum：该时间间隔内所有操作的总数。\n",
    "            # 将这些特征值添加到对应的列表中\n",
    "\n",
    "            for interval,group2 in group.groupby('interval'):\n",
    "                enrol_list.append(enrollment_id)\n",
    "                \n",
    "                interval_list.append(interval)\n",
    "                timelist = group2.time.tolist()\n",
    "                h1 = datetime.datetime.strptime(timelist[0],'%H:%M:%S')\n",
    "                h2 = datetime.datetime.strptime(timelist[len(timelist)-1],'%H:%M:%S')\n",
    "                hh = h2-h1\n",
    "                last_minutes.append(hh.seconds/60+1)\n",
    "                valid_len = [0,0,0,0]\n",
    "                valid_len[0] = len(group2[group2.event=='problem'])\n",
    "                valid_len[1] = len(group2[group2.event=='video'])\n",
    "                valid_len[2] = len(group2[group2.event == 'wiki'])\n",
    "                valid_len[3] = len(group2[group2.event == 'discussion'])\n",
    "                valid_opnum.append(reduce(self.add,valid_len))\n",
    "                all_opnum.append(len(group2))\n",
    "\n",
    "        # 创建并保存新的DataFrame:\n",
    "\n",
    "        # 使用这些特征列表创建一个新的DataFramedf2。\n",
    "        # 调整df2的列顺序，使其与期望的顺序相匹配。\n",
    "        # 使用to_csv函数将df2保存到target_path指定的路径，其中index = False表示在保存时不包含DataFrame的索引。\n",
    "        df2 = pd.DataFrame({\"enrollment_id\":enrol_list,\"interval\":interval_list,\"last_minutes\":last_minutes,\"valid_opnum\":valid_opnum,\"all_opnum\":all_opnum})\n",
    "        df2 = df2[[\"enrollment_id\",\"interval\",\"last_minutes\",\"valid_opnum\",\"all_opnum\"]]\n",
    "        df2.to_csv(target_path,index=False)\n",
    "\n",
    "    ##op_of_day 用户操作时间\n",
    "    def op_of_day(self,source_path,target_path):\n",
    "        #从用户操作日志中提取每日的操作特征，并将这些特征写入一个新的CSV文件中。\n",
    "        # source_path = \"feature/log_feature#.csv\"\n",
    "        # target_path = \"feature/log_feature_final.csv\"\n",
    "\n",
    "        print(\"op_of_day...\")\n",
    "        log_file = codecs.open(source_path,'r','utf-8')\n",
    "        log_final_file = codecs.open(target_path,'w+','utf-8')\n",
    "        framedata1 = pd.read_csv(log_file)\n",
    "        writer = csv.writer(log_final_file,delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writedata = list()\n",
    "        for i in range(0,111):\n",
    "            writedata.append('')\n",
    "        writedata[0]=\"enrollment_id\"\n",
    "        index =1\n",
    "        for i in range(1,31):\n",
    "            writedata[i]=\"all_opnum_\"+str(i)\n",
    "            writedata[i+30]=\"valid_opnum_\"+str(i)\n",
    "            writedata[i+60]=\"last_minutes_\"+str(i)\n",
    "            index += 3\n",
    "        name_array1 = [\"pre\",\"mid\",\"last\",\"thirty_day\"]\n",
    "        name_array2 = [\"min\",\"max\",\"sum\",\"mean\",\"std\"]\n",
    "        for name1 in name_array1:\n",
    "            for name2 in name_array2:\n",
    "                writedata[index]=name1+\"_\"+name2\n",
    "                index += 1\n",
    "                \n",
    "        writer.writerow(writedata)    \n",
    "                \n",
    "        for enrollment_id,group in framedata1.groupby('enrollment_id'):\n",
    "            writedata[0]=enrollment_id\n",
    "            interval_list = group.interval.tolist()\n",
    "            last_minutes_list = group.last_minutes.tolist()\n",
    "            valid_num_list = group.valid_opnum.tolist()\n",
    "            all_num_list = group.all_opnum.tolist()\n",
    "            tag = 0\n",
    "            for i in range(1,31):\n",
    "                if i in interval_list:  #如果用户今天参加课程\n",
    "                    writedata[i] = all_num_list[tag] #第一个30特征，记录操作总次数\n",
    "                    writedata[i+30] = valid_num_list[tag] #第二个30特征，记录有效操作次数\n",
    "                    writedata[i+60] = last_minutes_list[tag] #第三个30特征，记录持续时间\n",
    "                    tag = tag + 1\n",
    "                else:   #如果用户今天没有操作\n",
    "                    writedata[i] = 0\n",
    "                    writedata[i+30] = 0\n",
    "                    writedata[i+60] = 0\n",
    "            tag = 0\n",
    "            \n",
    "            '''\n",
    "                            分前中后三个阶段统计总操作次数特征\n",
    "            '''\n",
    "            preall = list()\n",
    "            midall = list()\n",
    "            lastall = list()\n",
    "            for i in range(1, 31):\n",
    "                if i in interval_list:\n",
    "                    if i > 0 and i <= 10:\n",
    "                        preall.append(all_num_list[tag])\n",
    "                    if i > 10 and i <= 20:\n",
    "                        midall.append(all_num_list[tag])\n",
    "                    if i > 20 and i <= 30:\n",
    "                        lastall.append(all_num_list[tag])\n",
    "                    tag = tag + 1\n",
    "                else:\n",
    "                    if i > 0 and i <= 10:\n",
    "                        preall.append(0)\n",
    "                    if i > 10 and i <= 20:\n",
    "                        midall.append(0)\n",
    "                    if i > 20 and i <= 30:\n",
    "                        lastall.append(0)\n",
    "    \n",
    "            ########处理前十天的相关统计#######\n",
    "            writedata[91] = min(preall)   #前十天中最小的操作次数\n",
    "            writedata[92] = max(preall)     #前十天中最大的操作次数\n",
    "            writedata[93] = np.array(preall).sum()    #前十天的总操作总次数\n",
    "            writedata[94] = int(np.array(preall).mean())#前十天的平均次数\n",
    "            writedata[95] = Decimal(np.array(preall).std()).quantize(Decimal('0.00'))        #操作次数的标准差\n",
    "    \n",
    "            #########处理中间十天的相关统计#########\n",
    "            writedata[96] = min(midall)\n",
    "            writedata[97] = max(midall)\n",
    "            writedata[98] = np.array(midall).sum()\n",
    "            writedata[99] = int(np.array(midall).mean())\n",
    "            writedata[100]=  Decimal(np.array(midall).std()).quantize(Decimal('0.00'))\n",
    "    \n",
    "            ########处理后十天的相关统计############\n",
    "            writedata[101] = min(lastall)\n",
    "            writedata[102] = max(lastall)\n",
    "            writedata[103] = np.array(lastall).sum()\n",
    "            writedata[104] = int(np.array(lastall).mean())\n",
    "            writedata[105] =  Decimal(np.array(lastall).std()).quantize(Decimal('0.00'))\n",
    "            ########处理三十天的相关统计############\n",
    "            tag = 0\n",
    "            writedata[106] = min(all_num_list)\n",
    "            writedata[107] = max(all_num_list)\n",
    "            templist = all_num_list\n",
    "            writedata[108] = np.array(templist).sum()\n",
    "            for i in range(0,30-len(all_num_list)):\n",
    "                templist.append(0)\n",
    "            writedata[109] = int(np.array(templist).mean())\n",
    "            writedata[110] = Decimal(np.array(templist).std()).quantize(Decimal('0.00'))\n",
    "            #print ('正在处理中....',enrollment_id)\n",
    "            writer.writerow(writedata)  #写入文件\n",
    "\n",
    "\n",
    "    ##feature_all  所有的特征\n",
    "    def feature_all(self,source_path1,source_path2,target_path):\n",
    "        #合并两个CSV文件，并将合并后的结果保存到一个新的CSV文件中\n",
    "        # source_path1 = 'feature/log_feature_final.csv'\n",
    "        # source_path2 = 'feature/enrollment_nondrop_precent#.csv'\n",
    "        # target_path = \"feature/final_feature_all.csv\"\n",
    "\n",
    "        print(\"feature_all...\")\n",
    "        df1 = pd.read_csv(source_path1)\n",
    "        df2 = pd.read_csv(source_path2)\n",
    "        df3 = pd.merge(df1,df2,on=\"enrollment_id\",how=\"left\")\n",
    "        df3.to_csv(target_path,index=False)\n",
    "\n",
    "\n",
    "        ##ext_feature扩展特征\n",
    "    def ext_feature(self):\n",
    "        source_path='preprocess_data/enrollment_dropout#.csv'\n",
    "        target_path=\"feature/enrollment_nondrop_precent#.csv\"\n",
    "        self.nondrop_precent(source_path,target_path)\n",
    "        \n",
    "        source_path='preprocess_data/log_train_final#.csv'\n",
    "        target_path=\"feature/log_feature#.csv\"\n",
    "        self.op_character(source_path,target_path)\n",
    "        \n",
    "        source_path=\"feature/log_feature#.csv\"\n",
    "        target_path=\"feature/log_feature_final.csv\"\n",
    "        self.op_of_day(source_path,target_path)\n",
    "        \n",
    "        source_path1='feature/log_feature_final.csv'\n",
    "        source_path2='feature/enrollment_nondrop_precent#.csv'\n",
    "        target_path=\"feature/final_feature_all.csv\"\n",
    "        self.feature_all(source_path1,source_path2,target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df97476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import unicode_literals\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from itertools import *\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "##from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class DropoutPredict(object):\n",
    "    def loadData(self,filename):\n",
    "        print(\"loadData...\")\n",
    "        df1 = pd.read_csv(filename)\n",
    "        df1.drop([\"enrollment_id\",\"course_id\"],axis=1)\n",
    "        df2 = df1.drop(\"dropout\",inplace=False,axis=1)\n",
    "        x = df2.values #DataFrame的值组成的二维数组\n",
    "        x = scale(x) #去均值后规范化\n",
    "        y = np.ravel(df1[\"dropout\"])\n",
    "        return x,y\n",
    "            \n",
    "    def logistic_regression(self,x_train,y_train):\n",
    "        print(\"logistic_regression...\")\n",
    "        clf1 = LogisticRegression()\n",
    "        # y_pred = clf1.predict(x_train)\n",
    "        ## score1 = cross_validation.cross_val_score(clf1,x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "        score1 = cross_val_score(clf1, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "        x = [int(i) for i in range(1,11)]\n",
    "        y = score1\n",
    "        pl.ylabel(u'Accuracy')\n",
    "        pl.xlabel(u'times')\n",
    "        pl.plot(x,y,label='LogReg')\n",
    "        pl.legend()\n",
    "        pl.savefig(\"picture/LogReg.png\")\n",
    "        print (np.mean(score1))\n",
    "\n",
    "        \n",
    "    def svm(self,x_train,y_train):\n",
    "        print(\"svm...\")\n",
    "        clf2 = svm.LinearSVC(random_state=2016)\n",
    "       ## score2 = cross_validation.cross_val_score(clf2,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        score2 = cross_val_score(clf2, x_train, y_train, cv=10, scoring='accuracy')\n",
    "        #print score2\n",
    "        print ('The accuracy of linearSVM:')\n",
    "        print (np.mean(score2))\n",
    "        x = [int(i) for i in range(1, 11)]\n",
    "        y = score2\n",
    "        pl.ylabel(u'Accuracy')\n",
    "        pl.xlabel(u'times')\n",
    "        pl.plot(x, y,label='SVM')\n",
    "        pl.legend()\n",
    "        pl.savefig(\"picture/SVM.png\")\n",
    "        \n",
    "    def naive_bayes(self,x_train,y_train):\n",
    "        print(\"naive_bayes...\")      \n",
    "        clf3 = GaussianNB()\n",
    "       ## score3 =  cross_validation.cross_val_score(clf3,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        score3 = cross_val_score(clf3, x_train, y_train, cv=10, scoring='accuracy')\n",
    "        print (\"The accuracy of Naive Bayes:\")\n",
    "        print (np.mean(score3))\n",
    "        x = [int(i) for i in range(1, 11)]\n",
    "        y = score3\n",
    "        pl.ylabel(u'Accuracy')\n",
    "        pl.xlabel(u'times')\n",
    "        pl.plot(x, y,label='NB')\n",
    "        pl.legend()\n",
    "        pl.savefig(\"picture/NB.png\")  \n",
    "          \n",
    "    def decision_tree(self,x_train,y_train):\n",
    "        print(\"decision_tree...\") \n",
    "        clf4 = tree.DecisionTreeClassifier()\n",
    "       ## score4 = cross_validation.cross_val_score(clf4,x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "        score4 = cross_val_score(clf4, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "        print ('The accuracy of DT:')\n",
    "        print (np.mean(score4))\n",
    "        x = [int(i) for i in range(1, 11)]\n",
    "        y = score4\n",
    "        pl.ylabel(u'Accuracy')\n",
    "        pl.xlabel(u'times')\n",
    "        pl.plot(x, y,label='DT')\n",
    "        pl.legend()\n",
    "        pl.savefig(\"picture/DT.png\")\n",
    "        \n",
    "    def gradient_boosting(self,x_train,y_train):\n",
    "        print(\"gradient_boosting...\")     \n",
    "        clf5 = GradientBoostingClassifier()\n",
    "        ##score5 = cross_validation.cross_val_score(clf5,x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "        score5 = cross_val_score(clf5, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "        print ('The accuracy of GradientBoosting:')\n",
    "        print (np.mean(score5))\n",
    "        x = [int(i) for i in range(1, 11)]\n",
    "        y = score5\n",
    "        pl.ylabel(u'Accuracy')\n",
    "        pl.xlabel(u'times')\n",
    "        pl.plot(x, y,label='GBDT')\n",
    "        pl.legend()\n",
    "        pl.savefig(\"picture/GBDT.png\")\n",
    "    def mlp(self,x_train,y_train):   \n",
    "        print(\"mlp...\") \n",
    "        clf = MLPClassifier(hidden_layer_sizes=(1000,),\n",
    "                            activation='logistic', solver='sgd',\n",
    "                            learning_rate_init = 0.001, max_iter=100000)\n",
    "        score = cross_val_score(clf,x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "      ##  score = cross_validation.cross_val_score(clf, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "        print ('The accuracy of MLP:')\n",
    "        print (np.mean(score))\n",
    "        x = [int(i) for i in range(1, 11)]\n",
    "        y = score\n",
    "        pl.ylabel(u'Accuracy')\n",
    "        pl.xlabel(u'times')\n",
    "        pl.plot(x, y,label='MLP')\n",
    "        pl.legend()\n",
    "        pl.savefig(\"picture/MLP.png\")\n",
    "        \n",
    "    def random_forest(self,x_train,y_train): \n",
    "        print(\"random_forest...\")        \n",
    "        clf = RandomForestClassifier(n_estimators=100)   \n",
    "       ## score = cross_validation.cross_val_score(clf,x_train,y_train,cv=10,scoring=\"accuracy\")\n",
    "        ##维度10\n",
    "        score = cross_val_score(clf, x_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "        ##维度30\n",
    "        # score = cross_val_score(clf, x_train, y_train, cv=30, scoring=\"accuracy\")\n",
    "\n",
    "        print ('The accuracy of RandomForest:')\n",
    "        print (np.mean(score))\n",
    "\n",
    "        #x = [int(i) for i in range(1, 11)]\n",
    "        x = [int(i) for i in range(1, 31)]\n",
    "        y = score\n",
    "        pl.ylabel(u'Accuracy')\n",
    "        pl.xlabel(u'times')\n",
    "        pl.plot(x, y,label='RandForest')\n",
    "        pl.legend()\n",
    "        pl.savefig(\"picture/RandomForest.png\")\n",
    "        \n",
    "    def drop_predict(self):\n",
    "        ##调用函数展示图片\n",
    "\n",
    "        ##使用的是final_feature_all.csv\n",
    "        filename = 'feature/final_feature_all.csv'\n",
    "        x_train,y_train = self.loadData(filename)\n",
    "\n",
    "        self.logistic_regression(x_train,y_train)\n",
    "        self.svm(x_train,y_train)\n",
    "        self.naive_bayes(x_train,y_train)\n",
    "        self.decision_tree(x_train,y_train)\n",
    "        \n",
    "        self.gradient_boosting(x_train,y_train)\n",
    "        \n",
    "\n",
    "        self.mlp(x_train,y_train)\n",
    "        self.random_forest(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_trans import PreProcess\n",
    "\n",
    "from ext_feature import FeatureEngineering\n",
    "from dropout_predict import DropoutPredict\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    '''\n",
    "            程序入口，只需要执行该代码，即可完成数据预处理，特征抽取，预测分类\n",
    "            ##如果特征数据已经有了，可以把预处理部分注释，直接运行分类预测\n",
    "    '''\n",
    "    '''\n",
    "    #数据预处理\n",
    "    preprocess = PreProcess()\n",
    "    preprocess.data_trans()\n",
    "\n",
    "    #抽取特征\n",
    "    feature_engineering = FeatureEngineering()\n",
    "    feature_engineering.ext_feature()\n",
    "    '''\n",
    "    #预测分类\n",
    "    prediction = DropoutPredict()\n",
    "    prediction.drop_predict()\n",
    "    print(\"...done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00015fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08f742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2c588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
